{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Linear Models\n",
    "\n",
    "Working through tons of sci-kit learn models with the Titanic Dataset. This notebook will explore section 1.1 from the [scikit learn user guide](https://scikit-learn.org/stable/modules/linear_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'embarked' (str)\n",
      "Stored 'age' (str)\n",
      "Stored 'sex' (str)\n",
      "Stored 'fare' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Name          0\n",
       "Sex           0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Ticket        0\n",
       "Fare          0\n",
       "Cabin       687\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Pclass        0\n",
       "Name          0\n",
       "Sex           0\n",
       "Age          86\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Ticket        0\n",
       "Fare          1\n",
       "Cabin       327\n",
       "Embarked      0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_train' (DataFrame)\n",
      "Stored 'X_validate' (DataFrame)\n",
      "Stored 'y_train' (DataFrame)\n",
      "Stored 'y_validate' (DataFrame)\n",
      "Stored 'X_test' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Set read-in-data variables\n",
    "\n",
    "embarked = 'integers-zero'\n",
    "age = 'baby-zero-other-avg'\n",
    "sex = 'integer'\n",
    "fare = 'integer-floor'\n",
    "\n",
    "# Store variables to pass to other notebook\n",
    "\n",
    "%store embarked\n",
    "%store age\n",
    "%store sex\n",
    "%store fare\n",
    "\n",
    "# Run read-in-data notebook\n",
    "\n",
    "%run ./read-in-data.ipynb\n",
    "\n",
    "# Read in evaluate predictions methods\n",
    "\n",
    "%run ./evaluate-predictions.ipynb\n",
    "\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table \n",
    "\n",
    "It seems wise to track each model's performance on a table, let's define one up top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "Fit a linear model with coefficients to minimize the residual sum of squares between observed responses in dataset and results predicted by linear approximation. \n",
    "\n",
    "May have trouble with multicolinearity, meaning, if two different features are correlated, it'll f them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Score the model\n",
    "\n",
    "y_predict = reg.predict(X_validate)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "ols = evaluate_predicted_probabilities(y_predict, y_validate['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Sex  Age  SibSp  Parch  Fare  Embarked\n",
       "PassengerId                                                \n",
       "591               3    0   35      0      0     7         1\n",
       "132               3    0   20      0      0     7         1\n",
       "629               3    0   26      0      0     7         1\n",
       "196               1    1   58      0      0   146         2\n",
       "231               1    1   35      1      0    83         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07180545],\n",
       "       [0.15174975],\n",
       "       [0.11977203],\n",
       "       [0.89174405],\n",
       "       [0.89093657]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
      "[[-1.72324766e-01  5.03883065e-01 -5.32961975e-03 -4.96475003e-02\n",
      "  -2.18689005e-02  2.66394982e-04  5.69583558e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(list(X_train.columns.values))\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71649332]\n"
     ]
    }
   ],
   "source": [
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          AUC  BestAccuracy  BestThreshold\n",
      "Ordinary Least Squares  0.869         0.832           0.61\n"
     ]
    }
   ],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(ols[0], 3), np.round(ols[1], 3), ols[2]]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Ordinary Least Squares\"]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "In general, simple models are better than complex models and usually do not overfit. If we can prevent coefficients from getting too big (important), it's a more simple model. \n",
    "\n",
    "Ridge Regression is linear regression with a penalty on the size of coefficients. IE, we have an extra parameter, `alpha`, which controls the amount of \"shrinkage\", ie, how small we want each of the parameters. If we set `alpha` = 0, this is just Ordinary Least Squares. `alpha` = .1 is a tiny penalty. scikit-learn has a default of .5, but in theory, we could scale up alpha arbitarily large, but at some point, you just end up arbitrarily picking the tinest coefficients possible. \n",
    "\n",
    "Thus, we'll try a series of alphas here and see how accuracy and AUC changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha = 0.000000, AUC = 0.868831, Best Accuracy = 0.832402 with threshold = 0.610000\n",
      "For alpha = 0.100000, AUC = 0.868831, Best Accuracy = 0.832402 with threshold = 0.610000\n",
      "For alpha = 0.250000, AUC = 0.868831, Best Accuracy = 0.832402 with threshold = 0.610000\n",
      "For alpha = 0.500000, AUC = 0.868831, Best Accuracy = 0.832402 with threshold = 0.610000\n",
      "For alpha = 0.750000, AUC = 0.868693, Best Accuracy = 0.832402 with threshold = 0.620000\n",
      "For alpha = 1.000000, AUC = 0.868693, Best Accuracy = 0.832402 with threshold = 0.620000\n",
      "For alpha = 1.500000, AUC = 0.868831, Best Accuracy = 0.832402 with threshold = 0.620000\n",
      "For alpha = 2.000000, AUC = 0.868969, Best Accuracy = 0.832402 with threshold = 0.640000\n",
      "For alpha = 5.000000, AUC = 0.869107, Best Accuracy = 0.832402 with threshold = 0.600000\n",
      "For alpha = 10.000000, AUC = 0.870485, Best Accuracy = 0.832402 with threshold = 0.560000\n",
      "For alpha = 25.000000, AUC = 0.873932, Best Accuracy = 0.837989 with threshold = 0.570000\n",
      "For alpha = 50.000000, AUC = 0.875310, Best Accuracy = 0.843575 with threshold = 0.530000\n",
      "For alpha = 75.000000, AUC = 0.875034, Best Accuracy = 0.843575 with threshold = 0.540000\n",
      "For alpha = 100.000000, AUC = 0.875034, Best Accuracy = 0.837989 with threshold = 0.510000\n",
      "For alpha = 200.000000, AUC = 0.868969, Best Accuracy = 0.832402 with threshold = 0.420000\n",
      "For alpha = 500.000000, AUC = 0.863041, Best Accuracy = 0.826816 with threshold = 0.400000\n",
      "For alpha = 1000.000000, AUC = 0.857389, Best Accuracy = 0.815642 with threshold = 0.380000\n"
     ]
    }
   ],
   "source": [
    "# Just a selection of different alphas: \n",
    "alpha=[0, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 5, 10, 25, 50, 75, 100, 200, 500, 1000]\n",
    "\n",
    "# Set champion variables for searching:\n",
    "b_alpha = 0\n",
    "b_auc = 0\n",
    "b_acc = 0\n",
    "b_thresh = 0\n",
    "\n",
    "for a in alpha:\n",
    "    ridge = linear_model.Ridge(alpha=a).fit(X_train, y_train)\n",
    "\n",
    "    # Score the model\n",
    "    y_predict = ridge.predict(X_validate)\n",
    "    # Evaluate the model\n",
    "    ridge_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "    print(\"For alpha = %f, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "          (a, np.round(ridge_results[0], 6), np.round(ridge_results[1], 6), ridge_results[2]))\n",
    "    if b_acc < ridge_results[1]:\n",
    "        b_alpha = a\n",
    "        b_auc = ridge_results[0]\n",
    "        b_acc = ridge_results[1]\n",
    "        b_thresh = ridge_results[2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       AUC  BestAccuracy  BestThreshold\n",
      "Ordinary Least Squares               0.869         0.832           0.61\n",
      "Ridge Regression, alpha = 50.000000  0.875         0.844           0.53\n"
     ]
    }
   ],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(b_auc, 3), np.round(b_acc, 3), b_thresh]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Ridge Regression, alpha = %f\" % (b_alpha)]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Ridge Regression\n",
    "\n",
    "To be fair, we had a large alpha above because we've got not-normalized (between 0 and 1) features. Thus, alpha is arbitrarily large depending on the size of features. I'm betting a 50 for alpha is getting pretty dominated by the fare column. If we normalize the columns, alpha will be between 0 and 1 and help a lot. Let's try that, too. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha = 0.000000, AUC = 0.868831, Best Accuracy = 0.832402 with threshold = 0.610000\n",
      "For alpha = 0.100000, AUC = 0.869382, Best Accuracy = 0.832402 with threshold = 0.610000\n",
      "For alpha = 0.250000, AUC = 0.868142, Best Accuracy = 0.832402 with threshold = 0.590000\n",
      "For alpha = 0.500000, AUC = 0.868969, Best Accuracy = 0.826816 with threshold = 0.510000\n",
      "For alpha = 0.750000, AUC = 0.869520, Best Accuracy = 0.826816 with threshold = 0.410000\n",
      "For alpha = 1.000000, AUC = 0.869107, Best Accuracy = 0.832402 with threshold = 0.410000\n",
      "For alpha = 1.500000, AUC = 0.869245, Best Accuracy = 0.832402 with threshold = 0.400000\n",
      "For alpha = 2.000000, AUC = 0.869382, Best Accuracy = 0.826816 with threshold = 0.450000\n",
      "For alpha = 5.000000, AUC = 0.867728, Best Accuracy = 0.832402 with threshold = 0.430000\n",
      "For alpha = 10.000000, AUC = 0.869107, Best Accuracy = 0.832402 with threshold = 0.420000\n",
      "For alpha = 25.000000, AUC = 0.870210, Best Accuracy = 0.821229 with threshold = 0.400000\n",
      "For alpha = 50.000000, AUC = 0.870210, Best Accuracy = 0.787709 with threshold = 0.400000\n",
      "For alpha = 75.000000, AUC = 0.870072, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 100.000000, AUC = 0.869934, Best Accuracy = 0.743017 with threshold = 0.400000\n",
      "For alpha = 200.000000, AUC = 0.870072, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000, AUC = 0.869934, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000, AUC = 0.869934, Best Accuracy = 0.653631 with threshold = 0.400000\n"
     ]
    }
   ],
   "source": [
    "# Just a selection of different alphas: \n",
    "alpha=[0, 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 5, 10, 25, 50, 75, 100, 200, 500, 1000]\n",
    "\n",
    "# Set champion variables for searching:\n",
    "b_alpha = 0\n",
    "b_auc = 0\n",
    "b_acc = 0\n",
    "b_thresh = 0\n",
    "\n",
    "for a in alpha:\n",
    "    ridge = linear_model.Ridge(alpha=a, normalize=True).fit(X_train, y_train)\n",
    "\n",
    "    # Score the model\n",
    "    y_predict = ridge.predict(X_validate)\n",
    "    # Evaluate the model\n",
    "    ridge_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "    print(\"For alpha = %f, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "          (a, np.round(ridge_results[0], 6), np.round(ridge_results[1], 6), ridge_results[2]))\n",
    "    if b_acc < ridge_results[1]:\n",
    "        b_alpha = a\n",
    "        b_auc = ridge_results[0]\n",
    "        b_acc = ridge_results[1]\n",
    "        b_thresh = ridge_results[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, guess not. Let's still add the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                         0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000            0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000  0.869         0.832   \n",
      "\n",
      "                                               BestThreshold  \n",
      "Ordinary Least Squares                                  0.61  \n",
      "Ridge Regression, alpha = 50.000000                     0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000           0.61  \n"
     ]
    }
   ],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(b_auc, 3), np.round(b_acc, 3), b_thresh]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Normalized Ridge Regression, alpha = %f\" % (b_alpha)]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression \n",
    "\n",
    "Ridge Regression does not let coefficients go to zero (which means that a particular feature gets thrown away). Lasso, on the other hand, allows it. We still have an alpha property to check over, and we'll look at the coefficients to see whether some are getting dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha = 0.100000, AUC = 0.879653, Best Accuracy = 0.832402 with threshold = 0.380000\n",
      "New best model, coefs: \n",
      "[-0.          0.06377947 -0.0018196  -0.         -0.          0.00210653\n",
      "  0.        ]\n",
      "for features: \n",
      "['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
      "For alpha = 0.250000, AUC = 0.790460, Best Accuracy = 0.787709 with threshold = 0.420000\n",
      "For alpha = 0.500000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 0.750000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.000000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.500000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 5.000000, AUC = 0.763510, Best Accuracy = 0.754190 with threshold = 0.400000\n",
      "For alpha = 10.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n"
     ]
    }
   ],
   "source": [
    "# Just a selection of different alphas: \n",
    "alpha=[0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 5, 10, 25, 50, 75, 100, 200, 500, 1000]\n",
    "\n",
    "# Set champion variables for searching:\n",
    "b_alpha = 0\n",
    "b_auc = 0\n",
    "b_acc = 0\n",
    "b_thresh = 0\n",
    "\n",
    "for a in alpha:\n",
    "    lasso = linear_model.Lasso(alpha=a).fit(X_train, y_train)\n",
    "\n",
    "    # Score the model\n",
    "    y_predict = lasso.predict(X_validate)\n",
    "    # Evaluate the model\n",
    "    lasso_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "    print(\"For alpha = %f, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "          (a, np.round(lasso_results[0], 6), np.round(lasso_results[1], 6), lasso_results[2]))\n",
    "    if b_acc < ridge_results[1]:\n",
    "        print(\"New best model, coefs: \")\n",
    "        print(lasso.coef_)\n",
    "        print(\"for features: \")\n",
    "        print(list(X_train.columns.values))\n",
    "        b_alpha = a\n",
    "        b_auc = lasso_results[0]\n",
    "        b_acc = lasso_results[1]\n",
    "        b_thresh = lasso_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                         0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000            0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000  0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000             0.880         0.832   \n",
      "\n",
      "                                               BestThreshold  \n",
      "Ordinary Least Squares                                  0.61  \n",
      "Ridge Regression, alpha = 50.000000                     0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000           0.61  \n",
      "Lasso Regression, alpha = 0.100000                      0.38  \n"
     ]
    }
   ],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(b_auc, 3), np.round(b_acc, 3), b_thresh]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Lasso Regression, alpha = %f\" % (b_alpha)]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Lasso regression says that Sex + age + Fare is all that matters, and we've got the best AUC yet, with accuracy equivalent to OLS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net \n",
    "\n",
    "We got a higher accuracy from Ridge and a higher AUC from Lasson. Elastic Net does both things - it allows features to be gone to zero, but also penalizes features that are given too much weight. \n",
    "\n",
    "* Ridge did it's thing by minimizing the sum of squares of the weights times alpha (called L2). \n",
    "* Lasso did it's thing by minimizing the absolute value of the sum of the weights (called L1), which also allowed weights to go to zero.\n",
    "\n",
    "Both include a penalty alpha, but by combining, we also need a ratio. If I set the L1 ratio to 0, this is essentially Ridge Regression. If I set L1 to 1, this is just lasso regression. Thus, we'll do our previous list of alphas, and try L1 ratios between .1 and .9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha = 0.100000 and L1 Ratio = 0.100000, AUC = 0.879032, Best Accuracy = 0.843575 with threshold = 0.510000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.200000, AUC = 0.880962, Best Accuracy = 0.837989 with threshold = 0.490000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.300000, AUC = 0.887579, Best Accuracy = 0.837989 with threshold = 0.390000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.400000, AUC = 0.887717, Best Accuracy = 0.837989 with threshold = 0.390000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.500000, AUC = 0.888958, Best Accuracy = 0.837989 with threshold = 0.490000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.600000, AUC = 0.887579, Best Accuracy = 0.837989 with threshold = 0.480000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.700000, AUC = 0.890612, Best Accuracy = 0.837989 with threshold = 0.460000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.800000, AUC = 0.888200, Best Accuracy = 0.832402 with threshold = 0.350000\n",
      "For alpha = 0.100000 and L1 Ratio = 0.900000, AUC = 0.884478, Best Accuracy = 0.832402 with threshold = 0.370000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.100000, AUC = 0.883995, Best Accuracy = 0.832402 with threshold = 0.450000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.200000, AUC = 0.886890, Best Accuracy = 0.837989 with threshold = 0.440000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.300000, AUC = 0.886683, Best Accuracy = 0.832402 with threshold = 0.400000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.400000, AUC = 0.869451, Best Accuracy = 0.815642 with threshold = 0.380000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.500000, AUC = 0.790460, Best Accuracy = 0.776536 with threshold = 0.390000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.600000, AUC = 0.792804, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.700000, AUC = 0.792115, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.800000, AUC = 0.791288, Best Accuracy = 0.782123 with threshold = 0.400000\n",
      "For alpha = 0.250000 and L1 Ratio = 0.900000, AUC = 0.790874, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.100000, AUC = 0.884133, Best Accuracy = 0.837989 with threshold = 0.410000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.200000, AUC = 0.854287, Best Accuracy = 0.787709 with threshold = 0.370000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.300000, AUC = 0.792804, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.400000, AUC = 0.791288, Best Accuracy = 0.782123 with threshold = 0.400000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.500000, AUC = 0.790460, Best Accuracy = 0.787709 with threshold = 0.420000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.600000, AUC = 0.786738, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.700000, AUC = 0.781913, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.800000, AUC = 0.775434, Best Accuracy = 0.776536 with threshold = 0.400000\n",
      "For alpha = 0.500000 and L1 Ratio = 0.900000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.100000, AUC = 0.871519, Best Accuracy = 0.826816 with threshold = 0.380000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.200000, AUC = 0.792804, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.300000, AUC = 0.791150, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.400000, AUC = 0.786738, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.500000, AUC = 0.778191, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.600000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.700000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.800000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 0.750000 and L1 Ratio = 0.900000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.100000, AUC = 0.830852, Best Accuracy = 0.782123 with threshold = 0.390000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.200000, AUC = 0.791012, Best Accuracy = 0.782123 with threshold = 0.400000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.300000, AUC = 0.786738, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.400000, AUC = 0.775434, Best Accuracy = 0.776536 with threshold = 0.400000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.500000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.600000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.700000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.800000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.000000 and L1 Ratio = 0.900000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.100000, AUC = 0.792528, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.200000, AUC = 0.786738, Best Accuracy = 0.782123 with threshold = 0.410000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.300000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.400000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.500000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.600000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.700000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.800000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 1.500000 and L1 Ratio = 0.900000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.100000, AUC = 0.791012, Best Accuracy = 0.782123 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.200000, AUC = 0.775434, Best Accuracy = 0.776536 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.300000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.400000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.500000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.600000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.700000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.800000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 2.000000 and L1 Ratio = 0.900000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.100000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.410000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.200000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.300000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.400000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.500000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.600000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.700000, AUC = 0.763510, Best Accuracy = 0.776536 with threshold = 0.400000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.800000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 5.000000 and L1 Ratio = 0.900000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.100000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.200000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.300000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.400000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha = 10.000000 and L1 Ratio = 0.500000, AUC = 0.763510, Best Accuracy = 0.754190 with threshold = 0.400000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 10.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.100000, AUC = 0.763510, Best Accuracy = 0.770950 with threshold = 0.410000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.200000, AUC = 0.763510, Best Accuracy = 0.754190 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 25.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.100000, AUC = 0.763510, Best Accuracy = 0.748603 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.200000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 50.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.100000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.200000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 75.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.100000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.200000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 100.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.100000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.200000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 200.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.100000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.200000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 500.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.100000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.200000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.300000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.400000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.500000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.600000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.700000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.800000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n",
      "For alpha = 1000.000000 and L1 Ratio = 0.900000, AUC = 0.500000, Best Accuracy = 0.653631 with threshold = 0.400000\n"
     ]
    }
   ],
   "source": [
    "# Just a selection of different alphas: \n",
    "alpha=[0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 5, 10, 25, 50, 75, 100, 200, 500, 1000]\n",
    "l1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Set champion variables for searching:\n",
    "b_alpha = 0\n",
    "b_l1_ratio = 0 \n",
    "b_auc = 0\n",
    "b_acc = 0\n",
    "b_thresh = 0\n",
    "\n",
    "for a in alpha:\n",
    "    for l in l1: \n",
    "        \n",
    "        elastic = linear_model.ElasticNet(alpha=a, l1_ratio=l).fit(X_train, y_train)\n",
    "\n",
    "        # Score the model\n",
    "        y_predict = elastic.predict(X_validate)\n",
    "        # Evaluate the model\n",
    "        elastic_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "        print(\"For alpha = %f and L1 Ratio = %f, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "              (a, l, np.round(elastic_results[0], 6), np.round(elastic_results[1], 6), elastic_results[2]))\n",
    "        if b_acc < ridge_results[1]:\n",
    "            b_alpha = a\n",
    "            b_l1_ratio = l\n",
    "            b_auc = elastic_results[0]\n",
    "            b_acc = elastic_results[1]\n",
    "            b_thresh = elastic_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                         0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000            0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000  0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000             0.880         0.832   \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000    0.879         0.844   \n",
      "\n",
      "                                               BestThreshold  \n",
      "Ordinary Least Squares                                  0.61  \n",
      "Ridge Regression, alpha = 50.000000                     0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000           0.61  \n",
      "Lasso Regression, alpha = 0.100000                      0.38  \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000             0.51  \n"
     ]
    }
   ],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(b_auc, 3), np.round(b_acc, 3), b_thresh]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"ElasticNet, alpha = %f, L1 = %f\" % (b_alpha, b_l1_ratio)]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it just looks like Ridge performs better. Oh well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Leaving continuous predictions, Logistic Regression fits a logistic curve (S shaped curve) which represents the probability of each row, given input. It's sort of the same thing we were doing above, but instead of a straight line, we use a much better shape to predict probabilities. \n",
    "\n",
    "Stock Logistic Regression from scikit-learn has two different penalties (L1 and L2) to choose between, as well as a C parameter. But first...\n",
    "\n",
    "From [Stack Overflow](https://stackoverflow.com/questions/22851316/what-is-the-inverse-of-regularization-strength-in-logistic-regression-how-shoul), Regularization is applying a penalty to increasing magnitude of parameter values to reduce overfitting, like what we did with Ridge. Imma follow the [sciki-learn user's guide to Logistic Regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py) and test three values of C with both L1 and L2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C = 1.000000 and penalty = l1, AUC = 0.803074, Best Accuracy = 0.826816 with threshold = 0.000000\n",
      "For C = 1.000000 and penalty = l2, AUC = 0.803074, Best Accuracy = 0.826816 with threshold = 0.000000\n",
      "For C = 0.100000 and penalty = l1, AUC = 0.786945, Best Accuracy = 0.815642 with threshold = 0.000000\n",
      "For C = 0.100000 and penalty = l2, AUC = 0.787428, Best Accuracy = 0.821229 with threshold = 0.000000\n",
      "For C = 0.010000 and penalty = l1, AUC = 0.663634, Best Accuracy = 0.748603 with threshold = 0.000000\n",
      "For C = 0.010000 and penalty = l2, AUC = 0.708712, Best Accuracy = 0.787709 with threshold = 0.000000\n"
     ]
    }
   ],
   "source": [
    "b_C = 0\n",
    "b_l = ''\n",
    "b_auc = 0\n",
    "b_acc = 0\n",
    "b_thresh = 0\n",
    "\n",
    "# Set regularization parameter\n",
    "for i, C in enumerate((1, 0.1, 0.01)):\n",
    "    for j, l in enumerate(('l1', 'l2')):\n",
    "        logistic = linear_model.LogisticRegression(C=C, penalty=l).fit(X_train, np.ravel(y_train))\n",
    "        y_predict = logistic.predict(X_validate)\n",
    "        logistic_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "        print(\"For C = %f and penalty = %s, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "                      (C, l, np.round(logistic_results[0], 6), np.round(logistic_results[1], 6), logistic_results[2]))\n",
    "        if b_acc < logistic_results[1]:\n",
    "            b_C = a\n",
    "            b_l = l\n",
    "            b_auc = logistic_results[0]\n",
    "            b_acc = logistic_results[1]\n",
    "            b_thresh = logistic_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                              0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000                 0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000       0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000                  0.880         0.832   \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000         0.879         0.844   \n",
      "Logistic Regression, C = 1000.000000, penalty = l1  0.803         0.827   \n",
      "\n",
      "                                                    BestThreshold  \n",
      "Ordinary Least Squares                                       0.61  \n",
      "Ridge Regression, alpha = 50.000000                          0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000                0.61  \n",
      "Lasso Regression, alpha = 0.100000                           0.38  \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000                  0.51  \n",
      "Logistic Regression, C = 1000.000000, penalty = l1           0.00  \n"
     ]
    }
   ],
   "source": [
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(b_auc, 3), np.round(b_acc, 3), b_thresh]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Logistic Regression, C = %f, penalty = %s\" % (b_C, b_l)]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "SGD is a simple yet effective way to learn classifiers. It's better at sparse data than other problems, but can handle large datsets. So maybe this won't work well here, considering the small size of data from the Titanic dataset. \n",
    "\n",
    "Per scikit-learn's user guide, SGD is efficient and easy to implement, but it requires a number of hyperparameters and its sensitive to \"feature scaling\", meaning, if one feature has a large swing in values (ie, a series with values [1, 2, 25, 100, 1000, 5000, 20000, 1000000]). \n",
    "\n",
    "Thus, we may consider scaling our fare column to prevent it from dominating. \n",
    "\n",
    "Let's try a few different versions of SGD, no need to loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 789.45, NNZs: 7, Bias: -67.935012, T: 712, Avg. loss: 7296.649068\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1070.99, NNZs: 7, Bias: -77.170815, T: 1424, Avg. loss: 4147.544535\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1167.65, NNZs: 7, Bias: -77.302975, T: 2136, Avg. loss: 3615.938927\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1110.71, NNZs: 7, Bias: -67.704763, T: 2848, Avg. loss: 2013.302177\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1273.80, NNZs: 7, Bias: -85.639530, T: 3560, Avg. loss: 2759.430353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1045.74, NNZs: 7, Bias: -59.877776, T: 4272, Avg. loss: 1591.077129\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1312.02, NNZs: 7, Bias: -52.032191, T: 4984, Avg. loss: 1466.931249\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1012.80, NNZs: 7, Bias: -43.127190, T: 5696, Avg. loss: 1631.776501\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 996.76, NNZs: 7, Bias: -34.676094, T: 6408, Avg. loss: 1167.548992\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 958.74, NNZs: 7, Bias: -28.094284, T: 7120, Avg. loss: 947.360892\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 962.44, NNZs: 7, Bias: -36.361944, T: 7832, Avg. loss: 1019.610459\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 950.02, NNZs: 7, Bias: -24.646340, T: 8544, Avg. loss: 982.610736\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 903.37, NNZs: 7, Bias: -16.005524, T: 9256, Avg. loss: 612.027202\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 901.32, NNZs: 7, Bias: -20.588302, T: 9968, Avg. loss: 774.602197\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 876.83, NNZs: 7, Bias: -12.613310, T: 10680, Avg. loss: 727.011850\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 846.88, NNZs: 7, Bias: -10.218712, T: 11392, Avg. loss: 670.195018\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 827.78, NNZs: 7, Bias: -11.042497, T: 12104, Avg. loss: 766.368129\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 796.94, NNZs: 7, Bias: -10.464561, T: 12816, Avg. loss: 539.860697\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 781.46, NNZs: 7, Bias: -6.411014, T: 13528, Avg. loss: 395.661714\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 785.90, NNZs: 7, Bias: -6.313490, T: 14240, Avg. loss: 712.865363\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 777.84, NNZs: 7, Bias: -5.002012, T: 14952, Avg. loss: 534.343786\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 763.93, NNZs: 7, Bias: -1.280271, T: 15664, Avg. loss: 612.041595\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 752.12, NNZs: 7, Bias: -2.413679, T: 16376, Avg. loss: 582.212003\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 731.22, NNZs: 7, Bias: -0.759798, T: 17088, Avg. loss: 325.884637\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 721.02, NNZs: 7, Bias: -1.822203, T: 17800, Avg. loss: 479.421650\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 703.08, NNZs: 7, Bias: -0.776456, T: 18512, Avg. loss: 352.774683\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 687.96, NNZs: 7, Bias: 1.768142, T: 19224, Avg. loss: 379.062830\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 680.41, NNZs: 7, Bias: 4.687822, T: 19936, Avg. loss: 339.010234\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 665.43, NNZs: 7, Bias: 7.510158, T: 20648, Avg. loss: 327.766483\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 660.75, NNZs: 7, Bias: 11.600194, T: 21360, Avg. loss: 488.286349\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 655.46, NNZs: 7, Bias: 13.359458, T: 22072, Avg. loss: 380.898285\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 640.63, NNZs: 7, Bias: 15.018231, T: 22784, Avg. loss: 343.809776\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 633.83, NNZs: 7, Bias: 18.746824, T: 23496, Avg. loss: 330.822833\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 623.04, NNZs: 7, Bias: 17.927474, T: 24208, Avg. loss: 269.396127\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 617.56, NNZs: 7, Bias: 17.929937, T: 24920, Avg. loss: 365.576427\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 613.22, NNZs: 7, Bias: 19.084256, T: 25632, Avg. loss: 318.990868\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 599.32, NNZs: 7, Bias: 19.098204, T: 26344, Avg. loss: 201.528414\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 592.35, NNZs: 7, Bias: 22.312712, T: 27056, Avg. loss: 316.308522\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 587.04, NNZs: 7, Bias: 21.941474, T: 27768, Avg. loss: 284.484698\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 580.04, NNZs: 7, Bias: 22.312320, T: 28480, Avg. loss: 277.321178\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 573.72, NNZs: 7, Bias: 23.300413, T: 29192, Avg. loss: 278.155352\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 565.27, NNZs: 7, Bias: 21.677799, T: 29904, Avg. loss: 237.335285\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 559.62, NNZs: 7, Bias: 23.903763, T: 30616, Avg. loss: 245.707210\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 551.03, NNZs: 7, Bias: 20.771690, T: 31328, Avg. loss: 214.281773\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 541.06, NNZs: 7, Bias: 23.225982, T: 32040, Avg. loss: 197.821892\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 533.51, NNZs: 7, Bias: 23.504630, T: 32752, Avg. loss: 174.556378\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 523.43, NNZs: 7, Bias: 26.126854, T: 33464, Avg. loss: 180.387511\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 518.03, NNZs: 7, Bias: 24.123637, T: 34176, Avg. loss: 159.157466\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 512.74, NNZs: 7, Bias: 26.369706, T: 34888, Avg. loss: 216.048475\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 505.69, NNZs: 7, Bias: 26.639878, T: 35600, Avg. loss: 153.930601\n",
      "Total training time: 0.01 seconds.\n",
      "For SGD, AUC = 0.712503, Best Accuracy = 0.787709 with threshold = 0.000000\n",
      "                                                      AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                              0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000                 0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000       0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000                  0.880         0.832   \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000         0.879         0.844   \n",
      "Logistic Regression, C = 1000.000000, penalty = l1  0.803         0.827   \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...  0.713         0.788   \n",
      "\n",
      "                                                    BestThreshold  \n",
      "Ordinary Least Squares                                       0.61  \n",
      "Ridge Regression, alpha = 50.000000                          0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000                0.61  \n",
      "Lasso Regression, alpha = 0.100000                           0.38  \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000                  0.51  \n",
      "Logistic Regression, C = 1000.000000, penalty = l1           0.00  \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...           0.00  \n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDClassifier(loss=\"hinge\",\n",
    "                                 penalty=\"l2\",\n",
    "                                 max_iter = 50,\n",
    "                                 learning_rate='optimal',\n",
    "                                 verbose = True).fit(X_train, np.ravel(y_train))\n",
    "y_predict = sgd.predict(X_validate)\n",
    "sgd_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "print(\"For SGD, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "                      (np.round(sgd_results[0], 6), np.round(sgd_results[1], 6), sgd_results[2]))\n",
    "\n",
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(sgd_results[0], 3), np.round(sgd_results[1], 3), sgd_results[2]]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Stochastic Gradient Descent, hinge loss, l2 penalty\" ]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 817.15, NNZs: 7, Bias: -58.518774, T: 712, Avg. loss: 7454.264078\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1369.49, NNZs: 7, Bias: -67.184519, T: 1424, Avg. loss: 5615.615455\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1121.01, NNZs: 7, Bias: -62.824038, T: 2136, Avg. loss: 3084.103180\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1108.33, NNZs: 7, Bias: -38.238393, T: 2848, Avg. loss: 2570.885937\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1105.80, NNZs: 7, Bias: -28.584367, T: 3560, Avg. loss: 1805.513964\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1053.29, NNZs: 7, Bias: -16.709178, T: 4272, Avg. loss: 1498.079333\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1323.71, NNZs: 7, Bias: -11.733104, T: 4984, Avg. loss: 1600.752570\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1032.11, NNZs: 7, Bias: -17.077944, T: 5696, Avg. loss: 1070.563070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 994.06, NNZs: 7, Bias: -17.046251, T: 6408, Avg. loss: 1071.469899\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 981.14, NNZs: 7, Bias: -14.295900, T: 7120, Avg. loss: 1088.203237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 965.82, NNZs: 7, Bias: -9.399860, T: 7832, Avg. loss: 740.120542\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 918.31, NNZs: 7, Bias: -7.119520, T: 8544, Avg. loss: 1059.467018\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 892.16, NNZs: 7, Bias: -3.094335, T: 9256, Avg. loss: 608.692412\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 877.65, NNZs: 7, Bias: -5.064102, T: 9968, Avg. loss: 739.497021\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 857.24, NNZs: 7, Bias: -6.905355, T: 10680, Avg. loss: 617.769497\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 825.75, NNZs: 7, Bias: 3.986012, T: 11392, Avg. loss: 457.114448\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 810.32, NNZs: 7, Bias: 3.252099, T: 12104, Avg. loss: 668.735313\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 795.75, NNZs: 7, Bias: 10.760608, T: 12816, Avg. loss: 459.703462\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 779.06, NNZs: 7, Bias: 17.041556, T: 13528, Avg. loss: 678.247904\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 762.50, NNZs: 7, Bias: 15.734391, T: 14240, Avg. loss: 377.332724\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 751.04, NNZs: 7, Bias: 14.450264, T: 14952, Avg. loss: 461.054799\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 738.30, NNZs: 7, Bias: 14.482353, T: 15664, Avg. loss: 356.947322\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 729.48, NNZs: 7, Bias: 12.054386, T: 16376, Avg. loss: 465.583459\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 714.57, NNZs: 7, Bias: 18.826553, T: 17088, Avg. loss: 305.147462\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 703.16, NNZs: 7, Bias: 24.798267, T: 17800, Avg. loss: 341.110645\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 699.28, NNZs: 7, Bias: 26.945330, T: 18512, Avg. loss: 477.028721\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 707.03, NNZs: 7, Bias: 19.398114, T: 19224, Avg. loss: 513.946987\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 688.84, NNZs: 7, Bias: 26.252957, T: 19936, Avg. loss: 321.902824\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 677.30, NNZs: 7, Bias: 30.493332, T: 20648, Avg. loss: 377.280227\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 667.99, NNZs: 7, Bias: 29.596815, T: 21360, Avg. loss: 367.862624\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 662.96, NNZs: 7, Bias: 30.448804, T: 22072, Avg. loss: 370.778924\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 649.84, NNZs: 7, Bias: 29.588746, T: 22784, Avg. loss: 303.023930\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 643.83, NNZs: 7, Bias: 32.428350, T: 23496, Avg. loss: 343.947631\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 640.31, NNZs: 7, Bias: 32.449951, T: 24208, Avg. loss: 286.047691\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 621.60, NNZs: 7, Bias: 31.278316, T: 24920, Avg. loss: 308.161798\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 612.70, NNZs: 7, Bias: 33.910879, T: 25632, Avg. loss: 284.224032\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 605.77, NNZs: 7, Bias: 33.514449, T: 26344, Avg. loss: 362.593154\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 595.10, NNZs: 7, Bias: 37.116617, T: 27056, Avg. loss: 317.374993\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 589.74, NNZs: 7, Bias: 38.524427, T: 27768, Avg. loss: 302.769633\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 583.08, NNZs: 7, Bias: 39.543505, T: 28480, Avg. loss: 298.700692\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 586.93, NNZs: 7, Bias: 36.551128, T: 29192, Avg. loss: 256.144589\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 575.88, NNZs: 7, Bias: 37.234205, T: 29904, Avg. loss: 331.332221\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 568.47, NNZs: 7, Bias: 38.499511, T: 30616, Avg. loss: 213.849578\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 565.28, NNZs: 7, Bias: 40.363303, T: 31328, Avg. loss: 236.892427\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 559.52, NNZs: 7, Bias: 42.498818, T: 32040, Avg. loss: 212.406005\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 549.80, NNZs: 7, Bias: 42.479330, T: 32752, Avg. loss: 281.668655\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 545.09, NNZs: 7, Bias: 42.734382, T: 33464, Avg. loss: 259.403049\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 536.96, NNZs: 7, Bias: 43.863745, T: 34176, Avg. loss: 242.682540\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 529.25, NNZs: 7, Bias: 45.549393, T: 34888, Avg. loss: 233.251849\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 524.81, NNZs: 7, Bias: 46.908123, T: 35600, Avg. loss: 200.670352\n",
      "Total training time: 0.01 seconds.\n",
      "For SGD, AUC = 0.774538, Best Accuracy = 0.759777 with threshold = 0.000000\n",
      "                                                      AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                              0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000                 0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000       0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000                  0.880         0.832   \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000         0.879         0.844   \n",
      "Logistic Regression, C = 1000.000000, penalty = l1  0.803         0.827   \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...  0.713         0.788   \n",
      "Stochastic Gradient Descent, perceptron loss, e...  0.775         0.760   \n",
      "\n",
      "                                                    BestThreshold  \n",
      "Ordinary Least Squares                                       0.61  \n",
      "Ridge Regression, alpha = 50.000000                          0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000                0.61  \n",
      "Lasso Regression, alpha = 0.100000                           0.38  \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000                  0.51  \n",
      "Logistic Regression, C = 1000.000000, penalty = l1           0.00  \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...           0.00  \n",
      "Stochastic Gradient Descent, perceptron loss, e...           0.00  \n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDClassifier(loss=\"perceptron\",\n",
    "                                 penalty=\"l2\",\n",
    "                                 max_iter = 50,\n",
    "                                 learning_rate='optimal',\n",
    "                                 verbose = True).fit(X_train, np.ravel(y_train))\n",
    "y_predict = sgd.predict(X_validate)\n",
    "sgd_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "print(\"For SGD, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "                      (np.round(sgd_results[0], 6), np.round(sgd_results[1], 6), sgd_results[2]))\n",
    "\n",
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(sgd_results[0], 3), np.round(sgd_results[1], 3), sgd_results[2]]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Stochastic Gradient Descent, perceptron loss, elasticnet penalty\" ]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 986.17, NNZs: 7, Bias: -53.984838, T: 712, Avg. loss: 6659.547630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2323.69, NNZs: 7, Bias: -40.222227, T: 1424, Avg. loss: 3645.893502\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1490.47, NNZs: 7, Bias: 27.928699, T: 2136, Avg. loss: 3145.513090\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1760.93, NNZs: 7, Bias: 27.281109, T: 2848, Avg. loss: 3261.976373\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1882.04, NNZs: 7, Bias: 38.218660, T: 3560, Avg. loss: 2563.288433\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1959.54, NNZs: 7, Bias: 62.047833, T: 4272, Avg. loss: 1175.288884\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2023.43, NNZs: 7, Bias: 61.562404, T: 4984, Avg. loss: 1813.101415\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2069.94, NNZs: 7, Bias: 56.897468, T: 5696, Avg. loss: 1291.766712\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2123.80, NNZs: 7, Bias: 58.171464, T: 6408, Avg. loss: 1200.617528\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2161.57, NNZs: 7, Bias: 62.120079, T: 7120, Avg. loss: 1209.904138\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2169.21, NNZs: 7, Bias: 58.856650, T: 7832, Avg. loss: 1007.617570\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2175.13, NNZs: 7, Bias: 65.400532, T: 8544, Avg. loss: 758.540310\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2178.33, NNZs: 7, Bias: 58.676935, T: 9256, Avg. loss: 841.458819\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2178.20, NNZs: 7, Bias: 67.326158, T: 9968, Avg. loss: 667.954495\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2187.73, NNZs: 7, Bias: 71.750641, T: 10680, Avg. loss: 628.481265\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2181.90, NNZs: 7, Bias: 75.075010, T: 11392, Avg. loss: 898.365978\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2180.71, NNZs: 7, Bias: 76.716264, T: 12104, Avg. loss: 606.264906\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2186.48, NNZs: 7, Bias: 80.629335, T: 12816, Avg. loss: 644.201529\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2166.84, NNZs: 7, Bias: 82.770266, T: 13528, Avg. loss: 575.078486\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2159.69, NNZs: 7, Bias: 84.787692, T: 14240, Avg. loss: 438.870421\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2171.79, NNZs: 7, Bias: 91.746158, T: 14952, Avg. loss: 594.274212\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2147.24, NNZs: 7, Bias: 94.071160, T: 15664, Avg. loss: 691.943513\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2136.38, NNZs: 7, Bias: 95.215839, T: 16376, Avg. loss: 569.564573\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2137.40, NNZs: 7, Bias: 92.406168, T: 17088, Avg. loss: 433.380007\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2137.54, NNZs: 7, Bias: 94.080878, T: 17800, Avg. loss: 615.440437\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2126.91, NNZs: 7, Bias: 97.697953, T: 18512, Avg. loss: 390.098863\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2116.80, NNZs: 7, Bias: 97.687352, T: 19224, Avg. loss: 457.147758\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2105.19, NNZs: 7, Bias: 97.641948, T: 19936, Avg. loss: 345.918848\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2095.93, NNZs: 7, Bias: 100.929864, T: 20648, Avg. loss: 548.459589\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2086.15, NNZs: 7, Bias: 100.039182, T: 21360, Avg. loss: 409.105796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2077.26, NNZs: 7, Bias: 102.726982, T: 22072, Avg. loss: 375.836426\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2077.74, NNZs: 7, Bias: 101.458218, T: 22784, Avg. loss: 355.272912\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2063.00, NNZs: 7, Bias: 102.697334, T: 23496, Avg. loss: 396.313487\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2053.09, NNZs: 7, Bias: 105.116151, T: 24208, Avg. loss: 330.121118\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2044.54, NNZs: 7, Bias: 109.817381, T: 24920, Avg. loss: 331.670565\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2034.32, NNZs: 7, Bias: 114.390139, T: 25632, Avg. loss: 312.294961\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2026.31, NNZs: 7, Bias: 115.862389, T: 26344, Avg. loss: 321.140812\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2014.83, NNZs: 7, Bias: 117.690162, T: 27056, Avg. loss: 254.624397\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2004.14, NNZs: 7, Bias: 120.490638, T: 27768, Avg. loss: 321.488060\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1999.42, NNZs: 7, Bias: 118.750368, T: 28480, Avg. loss: 382.777119\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1992.04, NNZs: 7, Bias: 118.094412, T: 29192, Avg. loss: 339.979096\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1981.66, NNZs: 7, Bias: 121.391500, T: 29904, Avg. loss: 282.902945\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1975.36, NNZs: 7, Bias: 120.762285, T: 30616, Avg. loss: 371.766983\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1969.28, NNZs: 7, Bias: 122.664176, T: 31328, Avg. loss: 315.785699\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1961.27, NNZs: 7, Bias: 123.274955, T: 32040, Avg. loss: 289.369329\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1954.93, NNZs: 7, Bias: 124.467119, T: 32752, Avg. loss: 307.913802\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1946.46, NNZs: 7, Bias: 126.205065, T: 33464, Avg. loss: 270.657076\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1939.20, NNZs: 7, Bias: 126.784341, T: 34176, Avg. loss: 260.343010\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1929.50, NNZs: 7, Bias: 129.604914, T: 34888, Avg. loss: 272.387191\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1922.72, NNZs: 7, Bias: 129.350575, T: 35600, Avg. loss: 280.887147\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1913.20, NNZs: 7, Bias: 130.436829, T: 36312, Avg. loss: 259.946909\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1904.39, NNZs: 7, Bias: 130.985229, T: 37024, Avg. loss: 222.939852\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1899.83, NNZs: 7, Bias: 129.947600, T: 37736, Avg. loss: 330.834500\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 1891.57, NNZs: 7, Bias: 131.236203, T: 38448, Avg. loss: 287.323110\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 1886.00, NNZs: 7, Bias: 133.506824, T: 39160, Avg. loss: 317.571747\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 1878.51, NNZs: 7, Bias: 135.730188, T: 39872, Avg. loss: 290.743925\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 1870.10, NNZs: 7, Bias: 137.183557, T: 40584, Avg. loss: 199.127519\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 1864.92, NNZs: 7, Bias: 136.725253, T: 41296, Avg. loss: 290.098285\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 1858.56, NNZs: 7, Bias: 138.836063, T: 42008, Avg. loss: 240.638027\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 1850.30, NNZs: 7, Bias: 139.987322, T: 42720, Avg. loss: 207.352529\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 1844.04, NNZs: 7, Bias: 141.346456, T: 43432, Avg. loss: 253.632302\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 1839.12, NNZs: 7, Bias: 142.243294, T: 44144, Avg. loss: 263.381949\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 1831.43, NNZs: 7, Bias: 142.900878, T: 44856, Avg. loss: 217.787363\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 1825.31, NNZs: 7, Bias: 144.198988, T: 45568, Avg. loss: 231.735246\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 1818.23, NNZs: 7, Bias: 145.903733, T: 46280, Avg. loss: 224.424782\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 1812.87, NNZs: 7, Bias: 147.367154, T: 46992, Avg. loss: 219.745398\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 1805.45, NNZs: 7, Bias: 148.193751, T: 47704, Avg. loss: 214.864078\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 1797.36, NNZs: 7, Bias: 149.623365, T: 48416, Avg. loss: 209.134800\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 1791.41, NNZs: 7, Bias: 150.224812, T: 49128, Avg. loss: 192.333723\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 1784.65, NNZs: 7, Bias: 152.603453, T: 49840, Avg. loss: 191.896493\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 1776.71, NNZs: 7, Bias: 153.766345, T: 50552, Avg. loss: 216.224724\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 1770.56, NNZs: 7, Bias: 154.726029, T: 51264, Avg. loss: 217.806715\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 1764.42, NNZs: 7, Bias: 155.489939, T: 51976, Avg. loss: 209.145292\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 1757.51, NNZs: 7, Bias: 157.550575, T: 52688, Avg. loss: 218.514823\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 1753.13, NNZs: 7, Bias: 157.547354, T: 53400, Avg. loss: 226.410360\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 1745.44, NNZs: 7, Bias: 159.369781, T: 54112, Avg. loss: 183.358769\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 1739.63, NNZs: 7, Bias: 160.275506, T: 54824, Avg. loss: 181.852707\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 1732.53, NNZs: 7, Bias: 161.875275, T: 55536, Avg. loss: 180.167938\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 1725.75, NNZs: 7, Bias: 162.571539, T: 56248, Avg. loss: 175.930824\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 1719.47, NNZs: 7, Bias: 162.403761, T: 56960, Avg. loss: 172.631247\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 1715.67, NNZs: 7, Bias: 162.227179, T: 57672, Avg. loss: 236.868874\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 1709.11, NNZs: 7, Bias: 164.092246, T: 58384, Avg. loss: 214.076672\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 1703.60, NNZs: 7, Bias: 164.253580, T: 59096, Avg. loss: 186.953198\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 1697.74, NNZs: 7, Bias: 164.420299, T: 59808, Avg. loss: 201.903372\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1693.10, NNZs: 7, Bias: 164.417611, T: 60520, Avg. loss: 182.032581\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1688.23, NNZs: 7, Bias: 165.068559, T: 61232, Avg. loss: 197.168894\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1682.52, NNZs: 7, Bias: 165.711222, T: 61944, Avg. loss: 179.868163\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1676.77, NNZs: 7, Bias: 167.293045, T: 62656, Avg. loss: 183.544938\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1671.76, NNZs: 7, Bias: 167.604460, T: 63368, Avg. loss: 173.346289\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1666.37, NNZs: 7, Bias: 167.914739, T: 64080, Avg. loss: 200.728354\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1660.71, NNZs: 7, Bias: 168.991194, T: 64792, Avg. loss: 165.527329\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1655.35, NNZs: 7, Bias: 170.198440, T: 65504, Avg. loss: 172.030531\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1649.83, NNZs: 7, Bias: 170.646627, T: 66216, Avg. loss: 192.682816\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1644.74, NNZs: 7, Bias: 171.230506, T: 66928, Avg. loss: 183.904295\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1640.49, NNZs: 7, Bias: 171.226056, T: 67640, Avg. loss: 217.942878\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1635.49, NNZs: 7, Bias: 171.521665, T: 68352, Avg. loss: 191.964007\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1630.97, NNZs: 7, Bias: 172.956154, T: 69064, Avg. loss: 192.057299\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1624.98, NNZs: 7, Bias: 173.665397, T: 69776, Avg. loss: 173.125764\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1619.45, NNZs: 7, Bias: 174.648542, T: 70488, Avg. loss: 200.483084\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1614.75, NNZs: 7, Bias: 174.508631, T: 71200, Avg. loss: 163.974361\n",
      "Total training time: 0.03 seconds.\n",
      "For SGD, AUC = 0.555004, Best Accuracy = 0.675978 with threshold = 0.000000\n",
      "                                                      AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                              0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000                 0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000       0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000                  0.880         0.832   \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000         0.879         0.844   \n",
      "Logistic Regression, C = 1000.000000, penalty = l1  0.803         0.827   \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...  0.713         0.788   \n",
      "Stochastic Gradient Descent, perceptron loss, e...  0.775         0.760   \n",
      "Stochastic Gradient Descent, hinge loss, no pen...  0.555         0.676   \n",
      "\n",
      "                                                    BestThreshold  \n",
      "Ordinary Least Squares                                       0.61  \n",
      "Ridge Regression, alpha = 50.000000                          0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000                0.61  \n",
      "Lasso Regression, alpha = 0.100000                           0.38  \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000                  0.51  \n",
      "Logistic Regression, C = 1000.000000, penalty = l1           0.00  \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...           0.00  \n",
      "Stochastic Gradient Descent, perceptron loss, e...           0.00  \n",
      "Stochastic Gradient Descent, hinge loss, no pen...           0.00  \n"
     ]
    }
   ],
   "source": [
    "sgd = linear_model.SGDClassifier(loss=\"hinge\",\n",
    "                                 penalty=\"none\",\n",
    "                                 max_iter = 100,\n",
    "                                 learning_rate='optimal',\n",
    "                                 verbose = True).fit(X_train, np.ravel(y_train))\n",
    "y_predict = sgd.predict(X_validate)\n",
    "sgd_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "print(\"For SGD, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "                      (np.round(sgd_results[0], 6), np.round(sgd_results[1], 6), sgd_results[2]))\n",
    "\n",
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(sgd_results[0], 3), np.round(sgd_results[1], 3), sgd_results[2]]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Stochastic Gradient Descent, hinge loss, no penalty\" ]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just overfitting the heck out of it, so that's no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Just like SGD above, but no learning rate stuff, not regularized, and only updates itself on mistakes. So maybe better? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 182.93, NNZs: 7, Bias: -2.000000, T: 712, Avg. loss: 983.233146\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 222.23, NNZs: 7, Bias: 5.000000, T: 1424, Avg. loss: 716.651685\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 308.63, NNZs: 7, Bias: -1.000000, T: 2136, Avg. loss: 969.308989\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 399.35, NNZs: 7, Bias: -4.000000, T: 2848, Avg. loss: 923.832865\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 461.23, NNZs: 7, Bias: 6.000000, T: 3560, Avg. loss: 752.726124\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 542.27, NNZs: 7, Bias: 3.000000, T: 4272, Avg. loss: 721.436798\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 588.21, NNZs: 7, Bias: 9.000000, T: 4984, Avg. loss: 766.140449\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 687.67, NNZs: 7, Bias: 7.000000, T: 5696, Avg. loss: 1075.240169\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 767.95, NNZs: 7, Bias: 4.000000, T: 6408, Avg. loss: 1005.099719\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 868.77, NNZs: 7, Bias: 12.000000, T: 7120, Avg. loss: 866.554775\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 881.61, NNZs: 7, Bias: 18.000000, T: 7832, Avg. loss: 1098.214888\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 922.32, NNZs: 7, Bias: 20.000000, T: 8544, Avg. loss: 723.258427\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 978.44, NNZs: 7, Bias: 13.000000, T: 9256, Avg. loss: 789.550562\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 988.69, NNZs: 7, Bias: 21.000000, T: 9968, Avg. loss: 967.278090\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1025.04, NNZs: 7, Bias: 25.000000, T: 10680, Avg. loss: 852.221910\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1062.34, NNZs: 7, Bias: 23.000000, T: 11392, Avg. loss: 719.973315\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1100.52, NNZs: 7, Bias: 29.000000, T: 12104, Avg. loss: 613.080056\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1115.46, NNZs: 7, Bias: 34.000000, T: 12816, Avg. loss: 655.521067\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1261.86, NNZs: 7, Bias: 39.000000, T: 13528, Avg. loss: 845.171348\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1201.71, NNZs: 7, Bias: 40.000000, T: 14240, Avg. loss: 924.745787\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1274.73, NNZs: 7, Bias: 38.000000, T: 14952, Avg. loss: 739.383427\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1283.35, NNZs: 7, Bias: 39.000000, T: 15664, Avg. loss: 1086.573034\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1303.49, NNZs: 7, Bias: 45.000000, T: 16376, Avg. loss: 907.525281\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1349.49, NNZs: 7, Bias: 44.000000, T: 17088, Avg. loss: 945.411517\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1363.60, NNZs: 7, Bias: 56.000000, T: 17800, Avg. loss: 698.859551\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1407.63, NNZs: 7, Bias: 51.000000, T: 18512, Avg. loss: 962.831461\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1421.98, NNZs: 7, Bias: 58.000000, T: 19224, Avg. loss: 695.316011\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1475.07, NNZs: 7, Bias: 56.000000, T: 19936, Avg. loss: 896.443820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1507.16, NNZs: 7, Bias: 54.000000, T: 20648, Avg. loss: 1071.859551\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1517.00, NNZs: 7, Bias: 57.000000, T: 21360, Avg. loss: 630.174157\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1560.13, NNZs: 7, Bias: 45.000000, T: 22072, Avg. loss: 1087.491573\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1604.69, NNZs: 7, Bias: 51.000000, T: 22784, Avg. loss: 736.941011\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1582.48, NNZs: 7, Bias: 57.000000, T: 23496, Avg. loss: 712.580056\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1603.20, NNZs: 7, Bias: 63.000000, T: 24208, Avg. loss: 534.292135\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1625.90, NNZs: 7, Bias: 60.000000, T: 24920, Avg. loss: 969.870787\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1641.93, NNZs: 7, Bias: 63.000000, T: 25632, Avg. loss: 961.748596\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1671.23, NNZs: 7, Bias: 65.000000, T: 26344, Avg. loss: 868.134831\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1701.95, NNZs: 7, Bias: 64.000000, T: 27056, Avg. loss: 914.161517\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1734.10, NNZs: 7, Bias: 63.000000, T: 27768, Avg. loss: 817.414326\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1742.63, NNZs: 7, Bias: 66.000000, T: 28480, Avg. loss: 729.539326\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1781.79, NNZs: 7, Bias: 66.000000, T: 29192, Avg. loss: 811.446629\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1795.78, NNZs: 7, Bias: 67.000000, T: 29904, Avg. loss: 1027.616573\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1828.04, NNZs: 7, Bias: 71.000000, T: 30616, Avg. loss: 974.544944\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1867.90, NNZs: 7, Bias: 83.000000, T: 31328, Avg. loss: 1015.435393\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1900.89, NNZs: 7, Bias: 70.000000, T: 32040, Avg. loss: 882.360955\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1915.41, NNZs: 7, Bias: 74.000000, T: 32752, Avg. loss: 609.589888\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1910.64, NNZs: 7, Bias: 78.000000, T: 33464, Avg. loss: 665.151685\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1930.94, NNZs: 7, Bias: 86.000000, T: 34176, Avg. loss: 767.553371\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1950.41, NNZs: 7, Bias: 88.000000, T: 34888, Avg. loss: 844.127809\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1958.88, NNZs: 7, Bias: 92.000000, T: 35600, Avg. loss: 772.997191\n",
      "Total training time: 0.01 seconds.\n",
      "For Perceptron, AUC = 0.759000, Best Accuracy = 0.715000 with threshold = 0.000000\n",
      "                                                      AUC  BestAccuracy  \\\n",
      "Ordinary Least Squares                              0.869         0.832   \n",
      "Ridge Regression, alpha = 50.000000                 0.875         0.844   \n",
      "Normalized Ridge Regression, alpha = 0.000000       0.869         0.832   \n",
      "Lasso Regression, alpha = 0.100000                  0.880         0.832   \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000         0.879         0.844   \n",
      "Logistic Regression, C = 1000.000000, penalty = l1  0.803         0.827   \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...  0.713         0.788   \n",
      "Stochastic Gradient Descent, perceptron loss, e...  0.775         0.760   \n",
      "Stochastic Gradient Descent, hinge loss, no pen...  0.555         0.676   \n",
      "Perceptron, 50 iterations                           0.759         0.715   \n",
      "\n",
      "                                                    BestThreshold  \n",
      "Ordinary Least Squares                                       0.61  \n",
      "Ridge Regression, alpha = 50.000000                          0.53  \n",
      "Normalized Ridge Regression, alpha = 0.000000                0.61  \n",
      "Lasso Regression, alpha = 0.100000                           0.38  \n",
      "ElasticNet, alpha = 0.100000, L1 = 0.100000                  0.51  \n",
      "Logistic Regression, C = 1000.000000, penalty = l1           0.00  \n",
      "Stochastic Gradient Descent, hinge loss, l2 pen...           0.00  \n",
      "Stochastic Gradient Descent, perceptron loss, e...           0.00  \n",
      "Stochastic Gradient Descent, hinge loss, no pen...           0.00  \n",
      "Perceptron, 50 iterations                                    0.00  \n"
     ]
    }
   ],
   "source": [
    "perceptron = linear_model.Perceptron(verbose = 1, max_iter = 50).fit(X_train, np.ravel(y_train))\n",
    "y_predict = perceptron.predict(X_validate)\n",
    "perceptron_results = evaluate_predicted_probabilities(y_predict, y_validate['Survived'], print_roc=False)\n",
    "print(\"For Perceptron, AUC = %f, Best Accuracy = %f with threshold = %f\" %\n",
    "                      (np.round(perceptron_results[0], 3), np.round(perceptron_results[1], 3), perceptron_results[2]))\n",
    "\n",
    "results = results.append(pd.DataFrame(\n",
    "        [[np.round(perceptron_results[0], 3), np.round(perceptron_results[1], 3), perceptron_results[2]]], \n",
    "        columns=[\"AUC\",\"BestAccuracy\",\"BestThreshold\"], \n",
    "        index=[\"Perceptron, 50 iterations\" ]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission \n",
    "\n",
    "Okay, it appears Ridge Regression and Lasson Regression get us the best models. Let's prepare submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model Ridge Regression with alpha = 50: \n",
    "ridge = linear_model.Ridge(alpha=50).fit(X_train, y_train)\n",
    "\n",
    "# Create copy of  X_test\n",
    "ridge_submission = X_test.copy(deep=True)\n",
    "\n",
    "# Copy probabilities to ridge_submission\n",
    "ridge_submission['Survived_p'] = ridge.predict(X_test)\n",
    "\n",
    "# Apply threshold\n",
    "ridge_submission['Survived'] = ridge_submission.apply(lambda row: 1 if row.Survived_p >= 0.53 else 0, axis = 1)\n",
    "\n",
    "# Write to .csv file\n",
    "ridge_submission[['Survived']].to_csv(path_or_buf=\"data/ridge.csv\", \n",
    "                        index=True, \n",
    "                        header=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model Lasso Regression with alpha = 0.1: \n",
    "lasso = linear_model.Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "# Create copy of  X_test\n",
    "lasso_submission = X_test.copy(deep=True)\n",
    "\n",
    "# Copy probabilities to ridge_submission\n",
    "lasso_submission['Survived_p'] = lasso.predict(X_test)\n",
    "\n",
    "# Apply threshold\n",
    "lasso_submission['Survived'] = lasso_submission.apply(lambda row: 1 if row.Survived_p >= 0.38 else 0, axis = 1)\n",
    "\n",
    "# Write to .csv file\n",
    "lasso_submission[['Survived']].to_csv(path_or_buf=\"data/lasso.csv\", \n",
    "                        index=True, \n",
    "                        header=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
